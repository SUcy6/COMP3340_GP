{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"h5EjqPk_2hyz","executionInfo":{"status":"ok","timestamp":1668438124286,"user_tz":-480,"elapsed":3955,"user":{"displayName":"Tina Su","userId":"15593028871386659268"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","import torchvision\n","from torch.nn.parameter import Parameter\n","from torch import Tensor\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import tarfile\n","import os\n","import csv\n","import shutil"]},{"cell_type":"code","source":["class LP_ReLU1(nn.Module):\n","    def __init__(self, alpha):\n","        super(LP_ReLU1, self).__init__()\n","        # self.positive_flag = positive_flag\n","        self.a = alpha\n","        # self.C1 = beta\n","    def forward(self, x):\n","        # x = x.detach().numpy()\n","        x=x.clone().detach().requires_grad_(True)\n","\n","        if (x.all() <= 0.0):\n","            out = 0\n","\n","        if (x.all() <= self.a and x.all() > 0):\n","            out = x\n","\n","        if (x.all() > self.a):\n","            out = (self.a + 0.05*(x - self.a))\n","        out = torch.tensor(out)\n","        return out\n"],"metadata":{"id":"eFGkN4JGbM5f","executionInfo":{"status":"ok","timestamp":1668439891768,"user_tz":-480,"elapsed":350,"user":{"displayName":"Tina Su","userId":"15593028871386659268"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["class LP_ReLU1_helper(nn.Module):\n","    def __init__(self, alpha):\n","        super(LP_ReLU1_helper, self).__init__()\n","        self.a = alpha\n","        \n","    def forward(self, x):\n","        x0 = x.clone().detach()\n","        f1 = LP_ReLU1(alpha=self.a)\n","        out = f1(x0)\n","        # f2 = log_act(alpha=self.a)\n","        # out2 = f2(x0)\n","        # out = torch.where(np.greater(x0, 0), out1, out2)\n","        out = out.squeeze()\n","        out = torch.tensor(out)\n","        return out\n"],"metadata":{"id":"-Z67u1S4Z-Xw","executionInfo":{"status":"ok","timestamp":1668440534454,"user_tz":-480,"elapsed":362,"user":{"displayName":"Tina Su","userId":"15593028871386659268"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["\n","x = torch.rand(3, 3, 448, 448)\n","conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                           bias=False)\n","x = conv1(x)\n","bn1 = nn.BatchNorm2d(64)\n","x = bn1(x)\n","activation = LP_ReLU1_helper(alpha=6)\n","out = activation(x)\n","print(out.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YO8UgNZ-bEZf","executionInfo":{"status":"ok","timestamp":1668438700318,"user_tz":-480,"elapsed":303,"user":{"displayName":"Tina Su","userId":"15593028871386659268"}},"outputId":"25a2e9d3-66db-45c9-f53f-67e4fa8a5251"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 64, 224, 224])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bFUhG8BrvcF_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1sEGhkop2rQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imports\n","\n","# Import basic libraries\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from collections import OrderedDict\n","\n","# Import PyTorch\n","import torch # import main library\n","from torch.autograd import Variable\n","import torch.nn as nn # import modules\n","from torch.autograd import Function # import Function to create custom activations\n","from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n","from torch import optim # import optimizers for demonstrations\n","import torch.nn.functional as F # import torch functions\n","from torchvision import datasets, transforms # import transformations to use for demo\n","\n","# helper function to train a model\n","def train_model(model, trainloader):\n","    '''\n","    Function trains the model and prints out the training log.\n","    INPUT:\n","        model - initialized PyTorch model ready for training.\n","        trainloader - PyTorch dataloader for training data.\n","    '''\n","    #setup training\n","\n","    #define loss function\n","    criterion = nn.NLLLoss()\n","    #define learning rate\n","    learning_rate = 0.003\n","    #define number of epochs\n","    epochs = 5\n","    #initialize optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    #run training and print out the loss to make sure that we are actually fitting to the training set\n","    print('Training the model. Make sure that loss decreases after each epoch.\\n')\n","    for e in range(epochs):\n","        running_loss = 0\n","        for images, labels in trainloader:\n","            images = images.view(images.shape[0], -1)\n","            log_ps = model(images)\n","            loss = criterion(log_ps, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","        else:\n","            # print out the loss to make sure it is decreasing\n","            print(f\"Training loss: {running_loss}\")\n","\n","# simply define a silu function\n","def silu(input):\n","    '''\n","    Applies the Sigmoid Linear Unit (SiLU) function element-wise:\n","\n","        SiLU(x) = x * sigmoid(x)\n","    '''\n","    return input * torch.sigmoid(input) # use torch.sigmoid to make sure that we created the most efficient implemetation based on builtin PyTorch functions\n","\n","# create a class wrapper from PyTorch nn.Module, so\n","# the function now can be easily used in models\n","class SiLU(nn.Module):\n","    '''\n","    Applies the Sigmoid Linear Unit (SiLU) function element-wise:\n","\n","        SiLU(x) = x * sigmoid(x)\n","\n","    Shape:\n","        - Input: (N, *) where * means, any number of additional\n","          dimensions\n","        - Output: (N, *), same shape as the input\n","\n","    References:\n","        -  Related paper:\n","        https://arxiv.org/pdf/1606.08415.pdf\n","\n","    Examples:\n","        >>> m = silu()\n","        >>> input = torch.randn(2)\n","        >>> output = m(input)\n","\n","    '''\n","    def __init__(self):\n","        '''\n","        Init method.\n","        '''\n","        super().__init__() # init the base class\n","\n","    def forward(self, input):\n","        '''\n","        Forward pass of the function.\n","        '''\n","        return silu(input) # simply apply already implemented SiLU\n","\n","# create class for basic fully-connected deep neural network\n","class ClassifierSiLU(nn.Module):\n","    '''\n","    Demo classifier model class to demonstrate SiLU\n","    '''\n","    def __init__(self):\n","        super().__init__()\n","\n","        # initialize layers\n","        self.fc1 = nn.Linear(784, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 64)\n","        self.fc4 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        # make sure the input tensor is flattened\n","        x = x.view(x.shape[0], -1)\n","\n","        # apply silu function\n","        x = silu(self.fc1(x))\n","\n","        # apply silu function\n","        x = silu(self.fc2(x))\n","\n","        # apply silu function\n","        x = silu(self.fc3(x))\n","\n","        x = F.log_softmax(self.fc4(x), dim=1)\n","\n","        return x\n","\n","# Implementation of Soft Exponential activation function\n","class soft_exponential(nn.Module):\n","    '''\n","    Implementation of soft exponential activation.\n","\n","    Shape:\n","        - Input: (N, *) where * means, any number of additional\n","          dimensions\n","        - Output: (N, *), same shape as the input\n","\n","    Parameters:\n","        - alpha - trainable parameter\n","\n","    References:\n","        - See related paper:\n","        https://arxiv.org/pdf/1602.01321.pdf\n","\n","    Examples:\n","        >>> a1 = soft_exponential(256)\n","        >>> x = torch.randn(256)\n","        >>> x = a1(x)\n","    '''\n","    def __init__(self, in_features, alpha = None):\n","        '''\n","        Initialization.\n","        INPUT:\n","            - in_features: shape of the input\n","            - aplha: trainable parameter\n","            aplha is initialized with zero value by default\n","        '''\n","        super(soft_exponential,self).__init__()\n","        self.in_features = in_features\n","\n","        # initialize alpha\n","        if alpha == None:\n","            self.alpha = Parameter(torch.tensor(6.0)) # create a tensor out of alpha\n","        else:\n","            self.alpha = Parameter(torch.tensor(alpha)) # create a tensor out of alpha\n","\n","        self.alpha.requiresGrad = True # set requiresGrad to true!\n","\n","    def forward(self, x):\n","        '''\n","        Forward pass of the function.\n","        Applies the function to the input elementwise.\n","        '''\n","        # x = torch.flatten(x)\n","        # for i in range(len(x)):\n","        #   for j in range(len(x[i])):\n","        #         x0 = x[i][j]\n","\n","        #         if x0 <= 0.0:\n","        #           x0 = 0.0\n","        #         if x0 <= self.alpha and x0 > 0:\n","        #           x0 = x0\n","        #         if x0 > self.alpha:\n","        #           x0 = (self.alpha + 0.05*(x0 - self.alpha))\n","        \n","        # return x\n","        # unflatten = torch.nn.Unflatten(1, torch.Size([3, 64, 224, 224]))\n","        # x = unflatten(x)\n","          \n","        if (x.any() <= 0.0):\n","            return torch.zeros_like(x)\n","\n","        if (x.any() <= self.alpha and x.any() > 0):\n","            return x\n","\n","        if (x.any() > self.alpha):\n","            return (self.alpha + 0.05*(x - self.alpha))\n","\n","# create class for basic fully-connected deep neural network\n","class ClassifierSExp(nn.Module):\n","    '''\n","    Basic fully-connected network to test Soft Exponential activation.\n","    '''\n","    def __init__(self):\n","        super().__init__()\n","\n","        # initialize layers\n","        self.fc1 = nn.Linear(784, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 64)\n","        self.fc4 = nn.Linear(64, 10)\n","\n","        # initialize Soft Exponential activation\n","        self.a1 = soft_exponential(256)\n","        self.a2 = soft_exponential(128)\n","        self.a3 = soft_exponential(64)\n","\n","    def forward(self, x):\n","        # make sure the input tensor is flattened\n","        x = x.view(x.shape[0], -1)\n","\n","        # apply Soft Exponential unit\n","        x = self.a1(self.fc1(x))\n","        x = self.a2(self.fc2(x))\n","        x = self.a3(self.fc3(x))\n","        x = F.log_softmax(self.fc4(x), dim=1)\n","\n","        return x\n","\n","# Implementation of BReLU activation function with custom backward step\n","class brelu(Function):\n","    '''\n","    Implementation of BReLU activation function.\n","\n","    Shape:\n","        - Input: (N, *) where * means, any number of additional\n","          dimensions\n","        - Output: (N, *), same shape as the input\n","\n","    References:\n","        - See BReLU paper:\n","        https://arxiv.org/pdf/1709.04054.pdf\n","\n","    Examples:\n","        >>> brelu_activation = brelu.apply\n","        >>> t = torch.randn((5,5), dtype=torch.float, requires_grad = True)\n","        >>> t = brelu_activation(t)\n","    '''\n","    #both forward and backward are @staticmethods\n","    @staticmethod\n","    def forward(ctx, input):\n","        \"\"\"\n","        In the forward pass we receive a Tensor containing the input and return\n","        a Tensor containing the output. ctx is a context object that can be used\n","        to stash information for backward computation. You can cache arbitrary\n","        objects for use in the backward pass using the ctx.save_for_backward method.\n","        \"\"\"\n","        ctx.save_for_backward(input) # save input for backward pass\n","\n","        # get lists of odd and even indices\n","        input_shape = input.shape[0]\n","        even_indices = [i for i in range(0, input_shape, 2)]\n","        odd_indices = [i for i in range(1, input_shape, 2)]\n","\n","        # clone the input tensor\n","        output = input.clone()\n","\n","        # apply ReLU to elements where i mod 2 == 0\n","        output[even_indices] = output[even_indices].clamp(min=0)\n","\n","        # apply inversed ReLU to inversed elements where i mod 2 != 0\n","        output[odd_indices] = 0 - output[odd_indices] # reverse elements with odd indices\n","        output[odd_indices] = - output[odd_indices].clamp(min = 0) # apply reversed ReLU\n","\n","        return output\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        \"\"\"\n","        In the backward pass we receive a Tensor containing the gradient of the loss\n","        with respect to the output, and we need to compute the gradient of the loss\n","        with respect to the input.\n","        \"\"\"\n","        grad_input = None # set output to None\n","\n","        input, = ctx.saved_tensors # restore input from context\n","\n","        # check that input requires grad\n","        # if not requires grad we will return None to speed up computation\n","        if ctx.needs_input_grad[0]:\n","            grad_input = grad_output.clone()\n","\n","            # get lists of odd and even indices\n","            input_shape = input.shape[0]\n","            even_indices = [i for i in range(0, input_shape, 2)]\n","            odd_indices = [i for i in range(1, input_shape, 2)]\n","\n","            # set grad_input for even_indices\n","            grad_input[even_indices] = (input[even_indices] >= 0).float() * grad_input[even_indices]\n","\n","            # set grad_input for odd_indices\n","            grad_input[odd_indices] = (input[odd_indices] < 0).float() * grad_input[odd_indices]\n","\n","        return grad_input\n","\n","# Simple model to demonstrate BReLU\n","class ClassifierBReLU(nn.Module):\n","    '''\n","    Simple fully-connected classifier model to demonstrate BReLU activation.\n","    '''\n","    def __init__(self):\n","        super(ClassifierBReLU, self).__init__()\n","\n","        # initialize layers\n","        self.fc1 = nn.Linear(784, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 64)\n","        self.fc4 = nn.Linear(64, 10)\n","\n","        # create shortcuts for BReLU\n","        self.a1 = brelu.apply\n","        self.a2 = brelu.apply\n","        self.a3 = brelu.apply\n","\n","    def forward(self, x):\n","        # make sure the input tensor is flattened\n","        x = x.view(x.shape[0], -1)\n","\n","        # apply BReLU\n","        x = self.a1(self.fc1(x))\n","        x = self.a2(self.fc2(x))\n","        x = self.a3(self.fc3(x))\n","        x = F.log_softmax(self.fc4(x), dim=1)\n","\n","        return x\n","\n","def main():\n","    print('Loading the Fasion MNIST dataset.\\n')\n","\n","    # Define a transform\n","    transform = transforms.Compose([transforms.ToTensor()])\n","\n","    # Download and load the training data for Fashion MNIST\n","    trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n","    # trainset = datasets.CIFAR10('~/.pytorch/CIFAR10_data/', download=True, train=True, transform=transform)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n","\n","    # 1. SiLU demonstration with model created with Sequential\n","    # use SiLU with model created with Sequential\n","\n","    # initialize activation function\n","    activation_function = SiLU()\n","\n","    # Initialize the model using nn.Sequential\n","    model = nn.Sequential(OrderedDict([\n","                          ('fc1', nn.Linear(784, 256)),\n","                          ('activation1', activation_function), # use SiLU\n","                          ('fc2', nn.Linear(256, 128)),\n","                          ('bn2', nn.BatchNorm1d(num_features=128)),\n","                          ('activation2', activation_function), # use SiLU\n","                          ('dropout', nn.Dropout(0.3)),\n","                          ('fc3', nn.Linear(128, 64)),\n","                          ('bn3', nn.BatchNorm1d(num_features=64)),\n","                          ('activation3', activation_function), # use SiLU\n","                          ('logits', nn.Linear(64, 10)),\n","                          ('logsoftmax', nn.LogSoftmax(dim=1))]))\n","\n","    # Run training\n","    print('Training model with SiLU activation.\\n')\n","    # train_model(model, trainloader)\n","\n","    # 2. SiLU demonstration of a model defined as a nn.Module\n","\n","    # Create demo model\n","    model = ClassifierSiLU()\n","\n","    # Run training\n","    print('Training model with SiLU activation.\\n')\n","    # train_model(model, trainloader)\n","\n","    # 3. Soft Eponential function demonstration (with trainable parameter alpha)\n","\n","    # Create demo model\n","    model = ClassifierSExp()\n","\n","    # Run training\n","    print('Training model with Soft Exponential activation.\\n')\n","    train_model(model, trainloader)\n","\n","    # 4. BReLU activation function demonstration (with custom backward step)\n","\n","    # Create demo model\n","    model = ClassifierBReLU()\n","\n","    # Run training\n","    print('Training model with BReLU activation.\\n')\n","    train_model(model, trainloader)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Egr7LUx92rTY","executionInfo":{"status":"ok","timestamp":1668445248502,"user_tz":-480,"elapsed":86233,"user":{"displayName":"Tina Su","userId":"15593028871386659268"}},"outputId":"cc999def-1b96-41fc-f473-8a3ec78b66af"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the Fasion MNIST dataset.\n","\n","Training model with SiLU activation.\n","\n","Training model with SiLU activation.\n","\n","Training model with Soft Exponential activation.\n","\n","Training the model. Make sure that loss decreases after each epoch.\n","\n","Training loss: 556.5562156885862\n","Training loss: 468.1928484290838\n","Training loss: 448.35064136981964\n","Training loss: 441.4540691226721\n","Training loss: 425.68906961381435\n","Training model with BReLU activation.\n","\n","Training the model. Make sure that loss decreases after each epoch.\n","\n","Training loss: 570.8969507217407\n","Training loss: 433.26680885255337\n","Training loss: 399.40364254266024\n","Training loss: 383.17385813593864\n","Training loss: 364.9030570536852\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rcrgGrF82rVy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d5JCJbkE2rYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bMtOAF0J2rbV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZeCicghEvcI3"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[{"file_id":"1RHYjnSY3wltNA0mopnCbiB9AKh-zgwhI","timestamp":1668416690405}]}},"nbformat":4,"nbformat_minor":0}
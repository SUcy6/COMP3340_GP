{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"h5EjqPk_2hyz","executionInfo":{"status":"ok","timestamp":1668438124286,"user_tz":-480,"elapsed":3955,"user":{"displayName":"Tina Su","userId":"15593028871386659268"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","import torchvision\n","from torch.nn.parameter import Parameter\n","from torch import Tensor\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import tarfile\n","import os\n","import csv\n","import shutil"]},{"cell_type":"code","source":["# define new activation function\n","class LP_ReLU1(nn.Module):\n","    \n","    def __init__(self, in_features, alpha = None):\n","        '''\n","        Initialization.\n","        INPUT:\n","            - in_features: shape of the input\n","            - aplha: trainable parameter\n","            aplha is initialized with zero value by default\n","        '''\n","        super(LP_ReLU1,self).__init__()\n","        self.in_features = in_features\n","\n","        # initialize alpha\n","        if alpha == None:\n","            self.alpha = Parameter(torch.tensor(6.0)) # create a tensor out of alpha\n","        else:\n","            self.alpha = Parameter(torch.tensor(alpha)) # create a tensor out of alpha\n","            \n","        self.alpha.requiresGrad = True # set requiresGrad to true!\n","\n","    def forward(self, x:Tensor)->Tensor:\n","        '''\n","        Forward pass of the function.\n","        Applies the function to the input elementwise.\n","        '''\n","        if (x <= 0):\n","            return 0\n","\n","        if (x <= self.alpha and x > 0):\n","            return x\n","\n","        if (x > self.alpha):\n","            return (self.alpha + 0.05*(x - self.alpha))\n","\n"],"metadata":{"id":"DOqKjMR_8Dc5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class log_act(nn.Module):\n","    def __init__(self, alpha):\n","        super(log_act, self).__init__()\n","        # self.positive_flag = positive_flag\n","        self.a = alpha\n","        # self.C1 = beta\n","    def forward(self, x):\n","        # x = x.detach().numpy()\n","        x=x.clone().detach().requires_grad_(True)\n","\n","        if (x.all() <= 0.0):\n","            out = 0\n","\n","        if (x.all() <= self.a and x.all() > 0):\n","            out = x\n","\n","        if (x.all() > self.a):\n","            out = (self.a + 0.05*(x - self.a))\n","        out = torch.tensor(out)\n","        return out\n"],"metadata":{"id":"eFGkN4JGbM5f","executionInfo":{"status":"ok","timestamp":1668438693207,"user_tz":-480,"elapsed":448,"user":{"displayName":"Tina Su","userId":"15593028871386659268"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["class log_act_helper(nn.Module):\n","    def __init__(self, alpha):\n","        super(log_act_helper, self).__init__()\n","        self.a = alpha\n","        \n","    def forward(self, x):\n","        x0 = x.clone().detach()\n","        f1 = log_act(alpha=self.a)\n","        out = f1(x0)\n","        # f2 = log_act(alpha=self.a)\n","        # out2 = f2(x0)\n","        # out = torch.where(np.greater(x0, 0), out1, out2)\n","        # out = torch.tensor(out)\n","        return out\n"],"metadata":{"id":"-Z67u1S4Z-Xw","executionInfo":{"status":"ok","timestamp":1668438697793,"user_tz":-480,"elapsed":262,"user":{"displayName":"Tina Su","userId":"15593028871386659268"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["\n","x = torch.rand(3, 3, 448, 448)\n","conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                           bias=False)\n","x = conv1(x)\n","bn1 = nn.BatchNorm2d(64)\n","x = bn1(x)\n","activation = log_act_helper(alpha=0.2)\n","out = activation(x)\n","print(out.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YO8UgNZ-bEZf","executionInfo":{"status":"ok","timestamp":1668438700318,"user_tz":-480,"elapsed":303,"user":{"displayName":"Tina Su","userId":"15593028871386659268"}},"outputId":"25a2e9d3-66db-45c9-f53f-67e4fa8a5251"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 64, 224, 224])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[{"file_id":"1RHYjnSY3wltNA0mopnCbiB9AKh-zgwhI","timestamp":1668416690405}]}},"nbformat":4,"nbformat_minor":0}